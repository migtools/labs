:sectlinks:
:markup-in-source: verbatim,attributes,quotes
:OCP3_GUID: %ocp3_guid%
:OCP3_DOMAIN: %ocp3_domain%
:OCP3_SSH_USER: %ocp3_ssh_user%
:OCP3_PASSWORD: %ocp3_password%
:OCP3_VERSION: 3.11
:OCP4_GUID: %ocp4_guid%
:OCP4_DOMAIN: %ocp4_domain%
:OCP4_SSH_USER: %ocp4_ssh_user%
:OCP4_PASSWORD: %ocp4_password%

== Migrate File-Uploader Application

The first application that we are going to migrate is a `file-uploader` application. The application consists of several front-end pods backed by a single RWX-mounted OCS3/Gluster volume. A single service/route load-balances requests across the front-end pods.

_RWX is a ReadWriteMany Access Mode for persistent volumes in which the volume can be mounted as read-write by many nodes._

This application has been pre-deployed on the your OCP 3 cluster. We will be migrating this application to our OCP 4 cluster, including the state to OCS4/Ceph.

image:../screenshots/lab4/file-uploader-app.png[File Uploader App]

In the source OCP {OCP3_VERSION} cluster terminal, let’s examine the application. Execute the following commands:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pods -n file-uploader**
NAME                    READY   STATUS      RESTARTS   AGE
file-uploader-1-build   0/1     Completed   0          44m
file-uploader-1-gbn49   1/1     Running     0          42m
file-uploader-1-ggjhj   1/1     Running     0          42m
file-uploader-1-rwlbt   1/1     Running     0          42m
--------------------------------------------------------------------------------

We can see the 3 replicas running. Next, let’s take a look at the storage:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pvc -n file-uploader**
NAME                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
file-uploader-vol-claim   Bound    pvc-b660a697-6afe-11ea-a7fb-065ab8be0337   20Gi       RWX            glusterfs-storage   18h
--------------------------------------------------------------------------------

We can see that we do indeed have a 20Gi Gluster volume mounted RWX to our application pods.

Next, let’s get the route to the application, and bring up the webUI.

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get route -n file-uploader**
NAME            HOST/PORT                                               PATH   SERVICES        PORT       TERMINATION   WILDCARD
file-uploader   file-uploader-file-uploader.apps.{OCP3_GUID}.{OCP3_DOMAIN}          file-uploader   8080-tcp                 None
--------------------------------------------------------------------------------

image:../screenshots/lab4/file-uploader-ui.png[File Uploader UI]

Lastly, let’s verify that the file-uploader application is *not* running on our destination OCP 4 cluster. In the OCP4 terminal run the following command:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pods -n file-uploader**
No resources found in file-uploader namespace.
--------------------------------------------------------------------------------

=== Migrate with MTC

Next, let’s open up the migration UI. +
Again, if we do not have it open, in order to get the route run the following command on the destination OCP 4 cluster terminal:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get routes migration -n openshift-migration -o jsonpath='{.spec.host}{"\n"}'**
migration-openshift-migration.apps.cluster-{OCP4_GUID}.{OCP4_GUID}.{OCP4_DOMAIN}
--------------------------------------------------------------------------------

The screen should look something like:

image:../screenshots/lab2/mtcUI.png[MTC Main Screen]

IMPORTANT: Remember our goal is to migrate our application from OCP/OCS 3 to OCP/OCS 4. This means moving our data from Gluster to Ceph. We will show you how to do this in MTC when setting up our migration plan.

==== Add a Cluster

The first thing we want to do is add the source OCP cluster we wish to migrate the application from. From the `Clusters` tab, click the `Add cluster` button. Fill out the necessary information. +
We will need a Service Account Token in order for MTC to communicate with the destination cluster:

Run the following in the OCP 3 cluster terminal.

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc sa get-token migration-operator -n openshift-migration**
eyJhbGciOifsfsds8ahmtpZCI6IiJ9fdsfdseyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtaWciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibWlnLXRva2VuLTdxMnhjIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1pZyIsImt1YmVybmss7gc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQ5NjYyZjgxLWEzNDItMTFlOS05NGRjLTA2MDlkNjY4OTQyMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptaWc6bWlnIn0.Qhcv0cwP539nSxbhIHFNHen0PNXSfLgBiDMFqt6BvHZBLET_UK0FgwyDxnRYRnDAHdxAGHN3dHxVtwhu-idHKI-mKc7KnyNXDfWe5O0c1xWv63BbEvyXnTNvpJuW1ChUGCY04DBb6iuSVcUMi04Jy_sVez00FCQ56xMSFzy5nLW5QpLFiFOTj2k_4Krcjhs8dgf02dgfkkshshjfgfsdfdsfdsa8fdsgdsfd8fasfdaTScsu4lEDSbMY25rbpr-XqhGcGKwnU58qlmtJcBNT3uffKuxAdgbqa-4zt9cLFeyayTKmelc1MLswlOvu3vvJ2soFx9VzWdPbGRMsjZWWLvJ246oyzwykYlBunYJbX3D_uPfyqoKfzA
--------------------------------------------------------------------------------
We need to save the output of the 'get-token', that is the long string we will enter into the ui when we create a new cluster entry. +
*Cluster name* is an arbitrary name you set for your cluster. +
*URL* field is FQDN of your OCP3 Master API. +
*Service account token* is the token you got in the previous command. +
The last field is `Route to Image Registry`. This is provided so that you could include a route to the cluster's image registry, if you wanted to allow direct image migration.

When done, click `Add cluster`.

image:../screenshots/lab4/mtc-add-cluster.png[MTC Add Cluster]

You should see a `Connection successful` message. Click `Close`. +
Now you should see the source and destination clusters populated.

image:../screenshots/lab4/mtc-clusters-added.png[MTC Clusters Added]

==== Create a Migration Plan

Now that we have a replication repository specified and both the source and destination clusters defined, we can create a migration plan. From the `Migration plans` tab, click the `Add migration plan` button:

image:../screenshots/lab4/mtc-add-migration-plan.png[MTC Add Mig Plan]

This will launch the Migration plan creation wizard. +
Give your plan a name, select the source and destination clusters, and your replication repository.  Then click Next.

image:../screenshots/lab4/mtc-mig-plan-general.png[MTC Mig Plan General]

Select the `file-uploader` namespace/project, and click Next.

image:../screenshots/lab4/mtc-mig-plan-namespaces.png[MTC Mig Plan Namespaces]

Now the OCS3/Gluster persistent volume associated with our application is displayed. For this example, since we are migrating our storage from Gluster to Ceph, let’s choose `Copy` as our transfer type.  Click Next.

image:../screenshots/lab4/mtc-mig-plan-pvolumes.png[MTC Mig Plan PVs]

Next, we will need to select the `copy method` and `target storage class`. MTC will attempt to pre-select these for you as defaults. In our case, we want *Filesystem copy* and *ocs-storagecluster-cephfs*. Click Next.

image:../screenshots/lab4/mtc-mig-plan-copyoptions.png[MTC Mig Plan CopyOptions]

If we configured the direct image location when the cluster was added, we could choose to use the Direct image migration. In our case it will be unavailable. The rest can be left as is. Click Next.

image:../screenshots/lab4/mtc-mig-plan-migoptions.png[MTC Mig Plan Options]

We will examine Migration Hooks in a subsequent exercise, so we will skip this step and proceed.  Click Finish.

image:../screenshots/lab4/mtc-mig-plan-hooks.png[MTC Mig Plan Hooks]



After the migration plan has been successfully validated, you can click `Close`.

image:../screenshots/lab4/mtc-mig-plan-validation.png[MTC Mig Plan Validation]



==== Migrate the Application Workload

Now we can select `Migrate` or `Stage` on the application. Since we don’t care about downtime for this example, let’s select `Migrate`:

image:../screenshots/lab4/mtc-mig-plan-added.png[MTC Mig Plan Added]

Optionally you may choose to _not_ terminate the application on the source cluster. Leave it checked and select `Migrate`.

image:../screenshots/lab4/mtc-quiesce.png[MTC Quiesce]

The migration will progress with a progress bar showing each step in the process.

image:../screenshots/lab4/mtc-progress-bar.png[MTC Progress Bar]

Once done, you should see `Migration Succeeded` on the migration plan.

image:../screenshots/lab4/mtc-migration-complete.png[MTC Migration Complete]

=== Verify Migrated Application

In the destination OCP 4 cluster terminal, let’s execute the following commands:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pods -n file-uploader**
NAME                     READY   STATUS      RESTARTS   AGE
file-uploader-1-build    1/1     Running     0          96s
file-uploader-1-deploy   0/1     Completed   0          95s
file-uploader-1-rc49v    1/1     Running     0          93s
file-uploader-1-vf2pt    1/1     Running     0          93s
file-uploader-1-zbt6d    1/1     Running     0          93s
--------------------------------------------------------------------------------

We see that the file-uploader application is running.

Let’s check the storage:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pvc -n file-uploader**
NAME                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
file-uploader-vol-claim   Bound    pvc-ff900007-c557-404c-852e-fca8bb4a5123   20Gi       RWX            ocs-storagecluster-cephfs   2m23s
--------------------------------------------------------------------------------

We see that our 20Gi volume has been moved and is now running on Ceph.

Lastly, let’s grab the route and open up the WebUI in our browser.

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get route -n file-uploader**
NAME            HOST/PORT                                                                                PATH   SERVICES        PORT       TERMINATION   WILDCARD
file-uploader   file-uploader-file-uploader.apps.cluster-{OCP4_GUID}.{OCP4_GUID}.{OCP4_DOMAIN}         file-uploader   8080-tcp                 None
--------------------------------------------------------------------------------

image:../screenshots/lab4/file-uploader-destination.png[File-Uploader-Destination]

*Success!* You have now successfully migrated your first application using MTC.
